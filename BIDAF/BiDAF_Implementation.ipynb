{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiDAF\n",
    "\n",
    "This notebook implements one of the most important papers in NLP literature: [BiDAF](https://arxiv.org/abs/1611.01603) or Bidirectional Attention Flow for Machine Comprehension. The key issue that this paper tries to address is that of *early summarization* in all the earlier approches that use attention mechanisms. The attention mechanisms until then were used to obtain a fixed-size summarization of given values and query. This, according to the authors leads to early summarization and loss of information. Moreover, previously, attention was only calculated in only one direction. To improve upon these issues, the authors propose a *hierarchical, multi-stage network*.   \n",
    "> *Our attention layer is not used to summarize the context paragraph into a ﬁxed-size vector. Instead, the attention is computed for every time step, and the attended vector at each time step, along with the representations from previous layers, is allowed to ﬂow through to the subsequent modeling layer. *\n",
    "\n",
    "Let's get into the intricacies of the model.\n",
    "The flow of this notebook will be similar to the previous one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mMB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle, time\n",
    "import re, os, string, typing, gc, json\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from preprocess import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data:  442\n",
      "Data Keys:  dict_keys(['title', 'paragraphs'])\n",
      "Title:  University_of_Notre_Dame\n",
      "Length of data:  48\n",
      "Data Keys:  dict_keys(['title', 'paragraphs'])\n",
      "Title:  Super_Bowl_50\n",
      "--------------------------\n",
      "Train list len:  87599\n",
      "Valid list len:  34726\n"
     ]
    }
   ],
   "source": [
    "# load dataset json files\n",
    "TRAIN_SQUAD_FILE = './data/Squad/train-v1.1.json'\n",
    "VALID_SQUAD_FILE = './data/Squad/dev-v1.1.json'\n",
    "\n",
    "\n",
    "train_data = load_json(TRAIN_SQUAD_FILE)\n",
    "valid_data = load_json(VALID_SQUAD_FILE)\n",
    "\n",
    "# parse the json structure to return the data as a list of dictionaries\n",
    "\n",
    "train_list = parse_data(train_data)\n",
    "valid_list = parse_data(valid_data)\n",
    "print('--------------------------')\n",
    "\n",
    "\n",
    "\n",
    "print('Train list len: ',len(train_list))\n",
    "print('Valid list len: ',len(valid_list))\n",
    "\n",
    "# converting the lists into dataframes\n",
    "\n",
    "train_df = pd.DataFrame(train_list)\n",
    "valid_df = pd.DataFrame(valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>[515, 541]</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>[188, 213]</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>[279, 296]</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>[381, 420]</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>[92, 126]</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  5733be284776f41900661182   \n",
       "1  5733be284776f4190066117f   \n",
       "2  5733be284776f41900661180   \n",
       "3  5733be284776f41900661181   \n",
       "4  5733be284776f4190066117e   \n",
       "\n",
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question       label  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...  [515, 541]   \n",
       "1  What is in front of the Notre Dame Main Building?  [188, 213]   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...  [279, 296]   \n",
       "3                  What is the Grotto at Notre Dame?  [381, 420]   \n",
       "4  What sits on top of the Main Building at Notre...   [92, 126]   \n",
       "\n",
       "                                    answer  \n",
       "0               Saint Bernadette Soubirous  \n",
       "1                a copper statue of Christ  \n",
       "2                        the Main Building  \n",
       "3  a Marian place of prayer and reflection  \n",
       "4       a golden statue of the Virgin Mary  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    \n",
    "    def to_lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    df.context = df.context.apply(to_lower)\n",
    "    df.question = df.question.apply(to_lower)\n",
    "    df.answer = df.answer.apply(to_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_df(train_df)\n",
    "preprocess_df(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 362 ms, sys: 37.4 ms, total: 399 ms\n",
      "Wall time: 398 ms\n",
      "Number of sentences in dataset:  118822\n"
     ]
    }
   ],
   "source": [
    "# gather text to build vocabularies\n",
    "\n",
    "%time vocab_text = gather_text_for_vocab([train_df, valid_df])\n",
    "print(\"Number of sentences in dataset: \", len(vocab_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw-vocab: 97475\n",
      "vocab-length: 97477\n",
      "word2idx-length: 97477\n",
      "CPU times: user 40.4 s, sys: 213 ms, total: 40.6 s\n",
      "Wall time: 40.6 s\n",
      "----------------------------------\n",
      "raw-char-vocab: 1316\n",
      "char-vocab-intersect: 202\n",
      "char2idx-length: 204\n",
      "CPU times: user 1.81 s, sys: 130 ms, total: 1.94 s\n",
      "Wall time: 1.94 s\n"
     ]
    }
   ],
   "source": [
    "# build word and character-level vocabularies\n",
    "\n",
    "%time word2idx, idx2word, word_vocab = build_word_vocab(vocab_text)\n",
    "print(\"----------------------------------\")\n",
    "%time char2idx, char_vocab = build_char_vocab(vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 24s, sys: 291 ms, total: 1min 25s\n",
      "Wall time: 1min 25s\n",
      "CPU times: user 35.8 s, sys: 28.8 ms, total: 35.8 s\n",
      "Wall time: 35.8 s\n",
      "CPU times: user 16.2 s, sys: 0 ns, total: 16.2 s\n",
      "Wall time: 16.2 s\n",
      "CPU times: user 6.35 s, sys: 0 ns, total: 6.35 s\n",
      "Wall time: 6.35 s\n"
     ]
    }
   ],
   "source": [
    "# numericalize context and questions for training and validation set\n",
    "\n",
    "%time train_df['context_ids'] = train_df.context.apply(context_to_ids, word2idx=word2idx)\n",
    "%time valid_df['context_ids'] = valid_df.context.apply(context_to_ids, word2idx=word2idx)\n",
    "%time train_df['question_ids'] = train_df.question.apply(question_to_ids, word2idx=word2idx)\n",
    "%time valid_df['question_ids'] = valid_df.question.apply(question_to_ids, word2idx=word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of error indices: 1004\n",
      "Number of error indices: 433\n"
     ]
    }
   ],
   "source": [
    "# get indices with tokenization errors and drop those indices \n",
    "\n",
    "train_err = get_error_indices(train_df, idx2word)\n",
    "valid_err = get_error_indices(valid_df, idx2word)\n",
    "\n",
    "train_df.drop(train_err, inplace=True)\n",
    "valid_df.drop(valid_err, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get start and end positions of answers from the context\n",
    "# this is basically the label for training QA models\n",
    "\n",
    "train_label_idx = train_df.apply(index_answer, axis=1, idx2word=idx2word)\n",
    "valid_label_idx = valid_df.apply(index_answer, axis=1, idx2word=idx2word)\n",
    "\n",
    "train_df['label_idx'] = train_label_idx\n",
    "valid_df['label_idx'] = valid_label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump to pickle files\n",
    "\n",
    "train_df.to_pickle('./pickle_files/bidaftrain.pkl')\n",
    "valid_df.to_pickle('./pickle_files/bidafvalid.pkl')\n",
    "\n",
    "with open('./pickle_files/bidafw2id.pickle','wb') as handle:\n",
    "    pickle.dump(word2idx, handle)\n",
    "\n",
    "with open('./pickle_files/bidafc2id.pickle','wb') as handle:\n",
    "    pickle.dump(char2idx, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from pickle files\n",
    "\n",
    "\n",
    "train_df = pd.read_pickle('./pickle_files/bidaftrain.pkl')\n",
    "valid_df = pd.read_pickle('./pickle_files/bidafvalid.pkl')\n",
    "\n",
    "with open('./pickle_files/bidafw2id.pickle','rb') as handle:\n",
    "    word2idx = pickle.load(handle)\n",
    "with open('./pickle_files/bidafc2id.pickle','rb') as handle:\n",
    "    char2idx = pickle.load(handle)\n",
    "\n",
    "idx2word = {v:k for k,v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>answer</th>\n",
       "      <th>context_ids</th>\n",
       "      <th>question_ids</th>\n",
       "      <th>label_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>architecturally, the school has a catholic cha...</td>\n",
       "      <td>to whom did the virgin mary allegedly appear i...</td>\n",
       "      <td>[515, 541]</td>\n",
       "      <td>saint bernadette soubirous</td>\n",
       "      <td>[16248, 3, 2, 133, 40, 10, 544, 801, 5, 8708, ...</td>\n",
       "      <td>[9, 569, 25, 2, 2676, 849, 5986, 1082, 6, 8044...</td>\n",
       "      <td>[102, 104]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>architecturally, the school has a catholic cha...</td>\n",
       "      <td>what is in front of the notre dame main building?</td>\n",
       "      <td>[188, 213]</td>\n",
       "      <td>a copper statue of christ</td>\n",
       "      <td>[16248, 3, 2, 133, 40, 10, 544, 801, 5, 8708, ...</td>\n",
       "      <td>[11, 12, 6, 1201, 4, 2, 1258, 1195, 236, 300, 7]</td>\n",
       "      <td>[37, 41]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>architecturally, the school has a catholic cha...</td>\n",
       "      <td>the basilica of the sacred heart at notre dame...</td>\n",
       "      <td>[279, 296]</td>\n",
       "      <td>the main building</td>\n",
       "      <td>[16248, 3, 2, 133, 40, 10, 544, 801, 5, 8708, ...</td>\n",
       "      <td>[2, 4540, 4, 2, 3909, 1498, 31, 1258, 1195, 12...</td>\n",
       "      <td>[57, 59]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>architecturally, the school has a catholic cha...</td>\n",
       "      <td>what is the grotto at notre dame?</td>\n",
       "      <td>[381, 420]</td>\n",
       "      <td>a marian place of prayer and reflection</td>\n",
       "      <td>[16248, 3, 2, 133, 40, 10, 544, 801, 5, 8708, ...</td>\n",
       "      <td>[11, 12, 2, 19572, 31, 1258, 1195, 7]</td>\n",
       "      <td>[76, 82]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>architecturally, the school has a catholic cha...</td>\n",
       "      <td>what sits on top of the main building at notre...</td>\n",
       "      <td>[92, 126]</td>\n",
       "      <td>a golden statue of the virgin mary</td>\n",
       "      <td>[16248, 3, 2, 133, 40, 10, 544, 801, 5, 8708, ...</td>\n",
       "      <td>[11, 8826, 24, 402, 4, 2, 236, 300, 31, 1258, ...</td>\n",
       "      <td>[17, 23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87594</th>\n",
       "      <td>5735d259012e2f140011a09d</td>\n",
       "      <td>kathmandu metropolitan city (kmc), in order to...</td>\n",
       "      <td>in what us state did kathmandu first establish...</td>\n",
       "      <td>[229, 235]</td>\n",
       "      <td>oregon</td>\n",
       "      <td>[1729, 1145, 53, 22, 25335, 21, 3, 6, 238, 9, ...</td>\n",
       "      <td>[6, 11, 201, 79, 25, 1729, 44, 1387, 34, 196, ...</td>\n",
       "      <td>[38, 38]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87595</th>\n",
       "      <td>5735d259012e2f140011a09e</td>\n",
       "      <td>kathmandu metropolitan city (kmc), in order to...</td>\n",
       "      <td>what was yangon previously known as?</td>\n",
       "      <td>[414, 421]</td>\n",
       "      <td>rangoon</td>\n",
       "      <td>[1729, 1145, 53, 22, 25335, 21, 3, 6, 238, 9, ...</td>\n",
       "      <td>[11, 13, 18118, 1035, 92, 15, 7]</td>\n",
       "      <td>[71, 71]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87596</th>\n",
       "      <td>5735d259012e2f140011a09f</td>\n",
       "      <td>kathmandu metropolitan city (kmc), in order to...</td>\n",
       "      <td>with what belorussian city does kathmandu have...</td>\n",
       "      <td>[476, 481]</td>\n",
       "      <td>minsk</td>\n",
       "      <td>[1729, 1145, 53, 22, 25335, 21, 3, 6, 238, 9, ...</td>\n",
       "      <td>[23, 11, 49291, 53, 57, 1729, 39, 10, 806, 7]</td>\n",
       "      <td>[85, 85]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87597</th>\n",
       "      <td>5735d259012e2f140011a0a0</td>\n",
       "      <td>kathmandu metropolitan city (kmc), in order to...</td>\n",
       "      <td>in what year did kathmandu create its initial ...</td>\n",
       "      <td>[199, 203]</td>\n",
       "      <td>1975</td>\n",
       "      <td>[1729, 1145, 53, 22, 25335, 21, 3, 6, 238, 9, ...</td>\n",
       "      <td>[6, 11, 58, 25, 1729, 711, 46, 1622, 196, 806, 7]</td>\n",
       "      <td>[31, 31]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87598</th>\n",
       "      <td>5735d259012e2f140011a0a1</td>\n",
       "      <td>kathmandu metropolitan city (kmc), in order to...</td>\n",
       "      <td>what is kmc an initialism of?</td>\n",
       "      <td>[0, 27]</td>\n",
       "      <td>kathmandu metropolitan city</td>\n",
       "      <td>[1729, 1145, 53, 22, 25335, 21, 3, 6, 238, 9, ...</td>\n",
       "      <td>[11, 12, 25335, 34, 39058, 4, 7]</td>\n",
       "      <td>[0, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86595 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id  \\\n",
       "0      5733be284776f41900661182   \n",
       "1      5733be284776f4190066117f   \n",
       "2      5733be284776f41900661180   \n",
       "3      5733be284776f41900661181   \n",
       "4      5733be284776f4190066117e   \n",
       "...                         ...   \n",
       "87594  5735d259012e2f140011a09d   \n",
       "87595  5735d259012e2f140011a09e   \n",
       "87596  5735d259012e2f140011a09f   \n",
       "87597  5735d259012e2f140011a0a0   \n",
       "87598  5735d259012e2f140011a0a1   \n",
       "\n",
       "                                                 context  \\\n",
       "0      architecturally, the school has a catholic cha...   \n",
       "1      architecturally, the school has a catholic cha...   \n",
       "2      architecturally, the school has a catholic cha...   \n",
       "3      architecturally, the school has a catholic cha...   \n",
       "4      architecturally, the school has a catholic cha...   \n",
       "...                                                  ...   \n",
       "87594  kathmandu metropolitan city (kmc), in order to...   \n",
       "87595  kathmandu metropolitan city (kmc), in order to...   \n",
       "87596  kathmandu metropolitan city (kmc), in order to...   \n",
       "87597  kathmandu metropolitan city (kmc), in order to...   \n",
       "87598  kathmandu metropolitan city (kmc), in order to...   \n",
       "\n",
       "                                                question       label  \\\n",
       "0      to whom did the virgin mary allegedly appear i...  [515, 541]   \n",
       "1      what is in front of the notre dame main building?  [188, 213]   \n",
       "2      the basilica of the sacred heart at notre dame...  [279, 296]   \n",
       "3                      what is the grotto at notre dame?  [381, 420]   \n",
       "4      what sits on top of the main building at notre...   [92, 126]   \n",
       "...                                                  ...         ...   \n",
       "87594  in what us state did kathmandu first establish...  [229, 235]   \n",
       "87595               what was yangon previously known as?  [414, 421]   \n",
       "87596  with what belorussian city does kathmandu have...  [476, 481]   \n",
       "87597  in what year did kathmandu create its initial ...  [199, 203]   \n",
       "87598                      what is kmc an initialism of?     [0, 27]   \n",
       "\n",
       "                                        answer  \\\n",
       "0                   saint bernadette soubirous   \n",
       "1                    a copper statue of christ   \n",
       "2                            the main building   \n",
       "3      a marian place of prayer and reflection   \n",
       "4           a golden statue of the virgin mary   \n",
       "...                                        ...   \n",
       "87594                                   oregon   \n",
       "87595                                  rangoon   \n",
       "87596                                    minsk   \n",
       "87597                                     1975   \n",
       "87598              kathmandu metropolitan city   \n",
       "\n",
       "                                             context_ids  \\\n",
       "0      [16248, 3, 2, 133, 40, 10, 544, 801, 5, 8708, ...   \n",
       "1      [16248, 3, 2, 133, 40, 10, 544, 801, 5, 8708, ...   \n",
       "2      [16248, 3, 2, 133, 40, 10, 544, 801, 5, 8708, ...   \n",
       "3      [16248, 3, 2, 133, 40, 10, 544, 801, 5, 8708, ...   \n",
       "4      [16248, 3, 2, 133, 40, 10, 544, 801, 5, 8708, ...   \n",
       "...                                                  ...   \n",
       "87594  [1729, 1145, 53, 22, 25335, 21, 3, 6, 238, 9, ...   \n",
       "87595  [1729, 1145, 53, 22, 25335, 21, 3, 6, 238, 9, ...   \n",
       "87596  [1729, 1145, 53, 22, 25335, 21, 3, 6, 238, 9, ...   \n",
       "87597  [1729, 1145, 53, 22, 25335, 21, 3, 6, 238, 9, ...   \n",
       "87598  [1729, 1145, 53, 22, 25335, 21, 3, 6, 238, 9, ...   \n",
       "\n",
       "                                            question_ids   label_idx  \n",
       "0      [9, 569, 25, 2, 2676, 849, 5986, 1082, 6, 8044...  [102, 104]  \n",
       "1       [11, 12, 6, 1201, 4, 2, 1258, 1195, 236, 300, 7]    [37, 41]  \n",
       "2      [2, 4540, 4, 2, 3909, 1498, 31, 1258, 1195, 12...    [57, 59]  \n",
       "3                  [11, 12, 2, 19572, 31, 1258, 1195, 7]    [76, 82]  \n",
       "4      [11, 8826, 24, 402, 4, 2, 236, 300, 31, 1258, ...    [17, 23]  \n",
       "...                                                  ...         ...  \n",
       "87594  [6, 11, 201, 79, 25, 1729, 44, 1387, 34, 196, ...    [38, 38]  \n",
       "87595                   [11, 13, 18118, 1035, 92, 15, 7]    [71, 71]  \n",
       "87596      [23, 11, 49291, 53, 57, 1729, 39, 10, 806, 7]    [85, 85]  \n",
       "87597  [6, 11, 58, 25, 1729, 711, 46, 1622, 196, 806, 7]    [31, 31]  \n",
       "87598                   [11, 12, 25335, 34, 39058, 4, 7]      [0, 2]  \n",
       "\n",
       "[86595 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset:\n",
    "    '''\n",
    "    - Creates batches dynamically by padding to the length of largest example\n",
    "      in a given batch.\n",
    "    - Calulates character vectors for contexts and question.\n",
    "    - Returns tensors for training.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, batch_size):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n",
    "        self.data = data\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def make_char_vector(self, max_sent_len, max_word_len, sentence):\n",
    "        \n",
    "        char_vec = torch.ones(max_sent_len, max_word_len).type(torch.LongTensor)\n",
    "        \n",
    "        for i, word in enumerate(nlp(sentence, disable=['parser','tagger','ner'])):\n",
    "            for j, ch in enumerate(word.text):\n",
    "                char_vec[i][j] = char2idx.get(ch, 0)\n",
    "        \n",
    "        return char_vec    \n",
    "    \n",
    "    def get_span(self, text):\n",
    "        \n",
    "        text = nlp(text, disable=['parser','tagger','ner'])\n",
    "        span = [(w.idx, w.idx+len(w.text)) for w in text]\n",
    "\n",
    "        return span\n",
    "\n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        Creates batches of data and yields them.\n",
    "        \n",
    "        Each yield comprises of:\n",
    "        :padded_context: padded tensor of contexts for each batch \n",
    "        :padded_question: padded tensor of questions for each batch \n",
    "        :char_ctx & ques_ctx: character-level ids for context and question\n",
    "        :label: start and end index wrt context_ids\n",
    "        :context_text,answer_text: used while validation to calculate metrics\n",
    "        :ids: question_ids for evaluation\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        for batch in self.data:\n",
    "            \n",
    "            spans = []\n",
    "            ctx_text = []\n",
    "            answer_text = []\n",
    "            \n",
    "            for ctx in batch.context:\n",
    "                ctx_text.append(ctx)\n",
    "                spans.append(self.get_span(ctx))\n",
    "                \n",
    "            for ans in batch.answer:\n",
    "                answer_text.append(ans)\n",
    "            max_context_len = max([len(ctx) for ctx in batch.context_ids])\n",
    "            padded_context = torch.LongTensor(len(batch), max_context_len).fill_(1)\n",
    "            \n",
    "            for i, ctx in enumerate(batch.context_ids):\n",
    "                padded_context[i, :len(ctx)] = torch.LongTensor(ctx)\n",
    "             \n",
    "            max_word_ctx = 0\n",
    "            for context in batch.context:\n",
    "                for word in nlp(context, disable=['parser','tagger','ner']):\n",
    "                    if len(word.text) > max_word_ctx:\n",
    "                        max_word_ctx = len(word.text)\n",
    "            \n",
    "            char_ctx = torch.ones(len(batch), max_context_len, max_word_ctx).type(torch.LongTensor)\n",
    "            for i, context in enumerate(batch.context):\n",
    "                char_ctx[i] = self.make_char_vector(max_context_len, max_word_ctx, context)\n",
    "#             print(char_ctx)   \n",
    "#             break\n",
    "            \n",
    "            max_question_len = max([len(ques) for ques in batch.question_ids])\n",
    "            padded_question = torch.LongTensor(len(batch), max_question_len).fill_(1)\n",
    "            \n",
    "            for i, ques in enumerate(batch.question_ids):\n",
    "                padded_question[i, :len(ques)] = torch.LongTensor(ques)\n",
    "                \n",
    "            max_word_ques = 0\n",
    "            for question in batch.question:\n",
    "                for word in nlp(question, disable=['parser','tagger','ner']):\n",
    "                    if len(word.text) > max_word_ques:\n",
    "                        max_word_ques = len(word.text)\n",
    "            \n",
    "            char_ques = torch.ones(len(batch), max_question_len, max_word_ques).type(torch.LongTensor)\n",
    "            for i, question in enumerate(batch.question):\n",
    "                char_ques[i] = self.make_char_vector(max_question_len, max_word_ques, question)\n",
    "            \n",
    "            ids = list(batch.id)  \n",
    "            label = torch.LongTensor(list(batch.label_idx))\n",
    "            \n",
    "#             print(padded_context, padded_question, char_ctx, char_ques, label, ctx_text, answer_text, ids)\n",
    "            yield (padded_context, padded_question, char_ctx, char_ques, label, ctx_text, answer_text, ids)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[16248,     3,     2,  ...,     1,     1,     1],\n",
       "         [16248,     3,     2,  ...,     1,     1,     1],\n",
       "         [16248,     3,     2,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    2,   108,    12,  ...,     1,     1,     1],\n",
       "         [    2,   108,    12,  ...,     1,     1,     1],\n",
       "         [    2,   303,     4,  ...,     1,     1,     1]]),\n",
       " tensor([[    9,   569,    25,     2,  2676,   849,  5986,  1082,     6,  8044,\n",
       "              6, 19573,   230,     7,     1,     1],\n",
       "         [   11,    12,     6,  1201,     4,     2,  1258,  1195,   236,   300,\n",
       "              7,     1,     1,     1,     1,     1],\n",
       "         [    2,  4540,     4,     2,  3909,  1498,    31,  1258,  1195,    12,\n",
       "           7628,     9,    28,   764,     7,     1],\n",
       "         [   11,    12,     2, 19572,    31,  1258,  1195,     7,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1],\n",
       "         [   11,  8826,    24,   402,     4,     2,   236,   300,    31,  1258,\n",
       "           1195,     7,     1,     1,     1,     1],\n",
       "         [   37,    25,     2, 12578,  1326,     4,  1258,  1195,   378,  3326,\n",
       "              7,     1,     1,     1,     1,     1],\n",
       "         [   35,   124,    12,  1258,  1195,    18,     2, 40458,   414,     7,\n",
       "              1,     1,     1,     1,     1,     1],\n",
       "         [   11,    12,     2,  1224,   908,   824,    31,  1258,  1195,    95,\n",
       "              7,     1,     1,     1,     1,     1],\n",
       "         [   35,    38,   908,  1086,  3747,    26,   149,    31,  1258,  1195,\n",
       "              7,     1,     1,     1,     1,     1],\n",
       "         [    6,    11,    58,    25,     2,   908,   824,   197,  1299,   378,\n",
       "           1922,    31,  1258,  1195,     7,     1],\n",
       "         [   54,    12,     2,  1517,     4,     2,  4378,     4,     2,  1395,\n",
       "           1048,     7,     1,     1,     1,     1],\n",
       "         [   11,    12,     2,   476,  6114,     4,     2,  4378,     4,     2,\n",
       "           1395,  1048,     7,     1,     1,     1],\n",
       "         [   11,    12,     2,   860,   764,    31,  1258,  1195,     7,     1,\n",
       "              1,     1,     1,     1,     1,     1],\n",
       "         [   11,  1094,   374,    31, 28672,   247,    31,  1258,  1195,     7,\n",
       "              1,     1,     1,     1,     1,     1],\n",
       "         [   28,  1711,    25,  2377, 28673,   711,     7,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1],\n",
       "         [   35,    38, 23906,   375,  1577,    26,  1335,     6,     2,   303,\n",
       "              4,  1610,    31,  1258,  1195,     7]]),\n",
       " tensor([[[200, 128, 173,  ..., 178, 178,  27],\n",
       "          [ 18,   1,   1,  ...,   1,   1,   1],\n",
       "          [ 59,  71,  39,  ...,   1,   1,   1],\n",
       "          ...,\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
       " \n",
       "         [[200, 128, 173,  ..., 178, 178,  27],\n",
       "          [ 18,   1,   1,  ...,   1,   1,   1],\n",
       "          [ 59,  71,  39,  ...,   1,   1,   1],\n",
       "          ...,\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
       " \n",
       "         [[200, 128, 173,  ..., 178, 178,  27],\n",
       "          [ 18,   1,   1,  ...,   1,   1,   1],\n",
       "          [ 59,  71,  39,  ...,   1,   1,   1],\n",
       "          ...,\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 59,  71,  39,  ...,   1,   1,   1],\n",
       "          [ 75, 187, 123,  ...,   1,   1,   1],\n",
       "          [123,  64,   1,  ...,   1,   1,   1],\n",
       "          ...,\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
       " \n",
       "         [[ 59,  71,  39,  ...,   1,   1,   1],\n",
       "          [ 75, 187, 123,  ...,   1,   1,   1],\n",
       "          [123,  64,   1,  ...,   1,   1,   1],\n",
       "          ...,\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
       " \n",
       "         [[ 59,  71,  39,  ...,   1,   1,   1],\n",
       "          [173, 193, 178,  ...,   1,   1,   1],\n",
       "          [193, 141,   1,  ...,   1,   1,   1],\n",
       "          ...,\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1]]]),\n",
       " tensor([[[ 59, 193,   1,  ...,   1,   1,   1],\n",
       "          [ 20,  71, 193,  ...,   1,   1,   1],\n",
       "          [ 19, 123,  19,  ...,   1,   1,   1],\n",
       "          ...,\n",
       "          [148,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
       " \n",
       "         [[ 20,  71, 200,  ...,   1,   1,   1],\n",
       "          [123,  64,   1,  ...,   1,   1,   1],\n",
       "          [123, 187,   1,  ...,   1,   1,   1],\n",
       "          ...,\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
       " \n",
       "         [[ 59,  71,  39,  ...,   1,   1,   1],\n",
       "          [  5, 200,  64,  ...,   1,   1,   1],\n",
       "          [193, 141,   1,  ...,   1,   1,   1],\n",
       "          ...,\n",
       "          [ 64,  59, 128,  ...,   1,   1,   1],\n",
       "          [148,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 20,  71, 200,  ...,   1,   1,   1],\n",
       "          [123, 187,  19,  ..., 178,  64,   1],\n",
       "          [178, 123,  55,  ...,   1,   1,   1],\n",
       "          ...,\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
       " \n",
       "         [[ 20,  71, 123,  ...,   1,   1,   1],\n",
       "          [125, 128, 123,  ...,   1,   1,   1],\n",
       "          [ 19, 123,  19,  ...,   1,   1,   1],\n",
       "          ...,\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1],\n",
       "          [  1,   1,   1,  ...,   1,   1,   1]],\n",
       " \n",
       "         [[ 71, 193,  20,  ...,   1,   1,   1],\n",
       "          [  4, 200, 187,  ...,   1,   1,   1],\n",
       "          [  5,  64,   1,  ...,   1,   1,   1],\n",
       "          ...,\n",
       "          [187, 193,  59,  ...,   1,   1,   1],\n",
       "          [ 19, 200,   4,  ...,   1,   1,   1],\n",
       "          [148,   1,   1,  ...,   1,   1,   1]]]),\n",
       " tensor([[102, 104],\n",
       "         [ 37,  41],\n",
       "         [ 57,  59],\n",
       "         [ 76,  82],\n",
       "         [ 17,  23],\n",
       "         [ 49,  50],\n",
       "         [ 82,  82],\n",
       "         [108, 109],\n",
       "         [ 25,  25],\n",
       "         [162, 162],\n",
       "         [ 22,  22],\n",
       "         [ 29,  30],\n",
       "         [ 47,  48],\n",
       "         [ 70,  73],\n",
       "         [127, 130],\n",
       "         [ 81,  81]]),\n",
       " ['architecturally, the school has a catholic character. atop the main building\\'s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \"venite ad me omnes\". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive (and in a direct line that connects through 3 statues and the gold dome), is a simple, modern stone statue of mary.',\n",
       "  'architecturally, the school has a catholic character. atop the main building\\'s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \"venite ad me omnes\". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive (and in a direct line that connects through 3 statues and the gold dome), is a simple, modern stone statue of mary.',\n",
       "  'architecturally, the school has a catholic character. atop the main building\\'s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \"venite ad me omnes\". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive (and in a direct line that connects through 3 statues and the gold dome), is a simple, modern stone statue of mary.',\n",
       "  'architecturally, the school has a catholic character. atop the main building\\'s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \"venite ad me omnes\". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive (and in a direct line that connects through 3 statues and the gold dome), is a simple, modern stone statue of mary.',\n",
       "  'architecturally, the school has a catholic character. atop the main building\\'s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \"venite ad me omnes\". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive (and in a direct line that connects through 3 statues and the gold dome), is a simple, modern stone statue of mary.',\n",
       "  \"as at most other universities, notre dame's students run a number of news media outlets. the nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. begun as a one-page journal in september 1876, the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states. the other magazine, the juggler, is released twice a year and focuses on student literature and artwork. the dome yearbook is published annually. the newspapers have varying publication interests, with the observer published daily and mainly reporting university and other news, and staffed by students from both notre dame and saint mary's college. unlike scholastic and the dome, the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university. in 1987, when some students believed that the observer began to show a conservative bias, a liberal newspaper, common sense was published. likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper irish rover went into production. neither paper is published as often as the observer; however, all three are distributed to all students. finally, in spring 2008 an undergraduate journal for political science research, beyond politics, made its debut.\",\n",
       "  \"as at most other universities, notre dame's students run a number of news media outlets. the nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. begun as a one-page journal in september 1876, the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states. the other magazine, the juggler, is released twice a year and focuses on student literature and artwork. the dome yearbook is published annually. the newspapers have varying publication interests, with the observer published daily and mainly reporting university and other news, and staffed by students from both notre dame and saint mary's college. unlike scholastic and the dome, the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university. in 1987, when some students believed that the observer began to show a conservative bias, a liberal newspaper, common sense was published. likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper irish rover went into production. neither paper is published as often as the observer; however, all three are distributed to all students. finally, in spring 2008 an undergraduate journal for political science research, beyond politics, made its debut.\",\n",
       "  \"as at most other universities, notre dame's students run a number of news media outlets. the nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. begun as a one-page journal in september 1876, the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states. the other magazine, the juggler, is released twice a year and focuses on student literature and artwork. the dome yearbook is published annually. the newspapers have varying publication interests, with the observer published daily and mainly reporting university and other news, and staffed by students from both notre dame and saint mary's college. unlike scholastic and the dome, the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university. in 1987, when some students believed that the observer began to show a conservative bias, a liberal newspaper, common sense was published. likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper irish rover went into production. neither paper is published as often as the observer; however, all three are distributed to all students. finally, in spring 2008 an undergraduate journal for political science research, beyond politics, made its debut.\",\n",
       "  \"as at most other universities, notre dame's students run a number of news media outlets. the nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. begun as a one-page journal in september 1876, the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states. the other magazine, the juggler, is released twice a year and focuses on student literature and artwork. the dome yearbook is published annually. the newspapers have varying publication interests, with the observer published daily and mainly reporting university and other news, and staffed by students from both notre dame and saint mary's college. unlike scholastic and the dome, the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university. in 1987, when some students believed that the observer began to show a conservative bias, a liberal newspaper, common sense was published. likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper irish rover went into production. neither paper is published as often as the observer; however, all three are distributed to all students. finally, in spring 2008 an undergraduate journal for political science research, beyond politics, made its debut.\",\n",
       "  \"as at most other universities, notre dame's students run a number of news media outlets. the nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. begun as a one-page journal in september 1876, the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states. the other magazine, the juggler, is released twice a year and focuses on student literature and artwork. the dome yearbook is published annually. the newspapers have varying publication interests, with the observer published daily and mainly reporting university and other news, and staffed by students from both notre dame and saint mary's college. unlike scholastic and the dome, the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university. in 1987, when some students believed that the observer began to show a conservative bias, a liberal newspaper, common sense was published. likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper irish rover went into production. neither paper is published as often as the observer; however, all three are distributed to all students. finally, in spring 2008 an undergraduate journal for political science research, beyond politics, made its debut.\",\n",
       "  'the university is the major seat of the congregation of holy cross (albeit not its official headquarters, which are in rome). its main seminary, moreau seminary, is located on the campus across st. joseph lake from the main building. old college, the oldest building on campus and located near the shore of st. mary lake, houses undergraduate seminarians. retired priests and brothers reside in fatima house (a former retreat center), holy cross house, as well as columba hall near the grotto. the university through the moreau seminary has ties to theologian frederick buechner. while not catholic, buechner has praised writers from notre dame and moreau seminary created a buechner prize for preaching.',\n",
       "  'the university is the major seat of the congregation of holy cross (albeit not its official headquarters, which are in rome). its main seminary, moreau seminary, is located on the campus across st. joseph lake from the main building. old college, the oldest building on campus and located near the shore of st. mary lake, houses undergraduate seminarians. retired priests and brothers reside in fatima house (a former retreat center), holy cross house, as well as columba hall near the grotto. the university through the moreau seminary has ties to theologian frederick buechner. while not catholic, buechner has praised writers from notre dame and moreau seminary created a buechner prize for preaching.',\n",
       "  'the university is the major seat of the congregation of holy cross (albeit not its official headquarters, which are in rome). its main seminary, moreau seminary, is located on the campus across st. joseph lake from the main building. old college, the oldest building on campus and located near the shore of st. mary lake, houses undergraduate seminarians. retired priests and brothers reside in fatima house (a former retreat center), holy cross house, as well as columba hall near the grotto. the university through the moreau seminary has ties to theologian frederick buechner. while not catholic, buechner has praised writers from notre dame and moreau seminary created a buechner prize for preaching.',\n",
       "  'the university is the major seat of the congregation of holy cross (albeit not its official headquarters, which are in rome). its main seminary, moreau seminary, is located on the campus across st. joseph lake from the main building. old college, the oldest building on campus and located near the shore of st. mary lake, houses undergraduate seminarians. retired priests and brothers reside in fatima house (a former retreat center), holy cross house, as well as columba hall near the grotto. the university through the moreau seminary has ties to theologian frederick buechner. while not catholic, buechner has praised writers from notre dame and moreau seminary created a buechner prize for preaching.',\n",
       "  'the university is the major seat of the congregation of holy cross (albeit not its official headquarters, which are in rome). its main seminary, moreau seminary, is located on the campus across st. joseph lake from the main building. old college, the oldest building on campus and located near the shore of st. mary lake, houses undergraduate seminarians. retired priests and brothers reside in fatima house (a former retreat center), holy cross house, as well as columba hall near the grotto. the university through the moreau seminary has ties to theologian frederick buechner. while not catholic, buechner has praised writers from notre dame and moreau seminary created a buechner prize for preaching.',\n",
       "  'the college of engineering was established in 1920, however, early courses in civil and mechanical engineering were a part of the college of science since the 1870s. today the college, housed in the fitzpatrick, cushing, and stinson-remick halls of engineering, includes five departments of study – aerospace and mechanical engineering, chemical and biomolecular engineering, civil engineering and geological sciences, computer science and engineering, and electrical engineering – with eight b.s. degrees offered. additionally, the college offers five-year dual degree programs with the colleges of arts and letters and of business awarding additional b.a. and master of business administration (mba) degrees, respectively.'],\n",
       " ['saint bernadette soubirous',\n",
       "  'a copper statue of christ',\n",
       "  'the main building',\n",
       "  'a marian place of prayer and reflection',\n",
       "  'a golden statue of the virgin mary',\n",
       "  'september 1876',\n",
       "  'twice',\n",
       "  'the observer',\n",
       "  'three',\n",
       "  '1987',\n",
       "  'rome',\n",
       "  'moreau seminary',\n",
       "  'old college',\n",
       "  'retired priests and brothers',\n",
       "  'buechner prize for preaching',\n",
       "  'eight'],\n",
       " ['5733be284776f41900661182',\n",
       "  '5733be284776f4190066117f',\n",
       "  '5733be284776f41900661180',\n",
       "  '5733be284776f41900661181',\n",
       "  '5733be284776f4190066117e',\n",
       "  '5733bf84d058e614000b61be',\n",
       "  '5733bf84d058e614000b61bf',\n",
       "  '5733bf84d058e614000b61c0',\n",
       "  '5733bf84d058e614000b61bd',\n",
       "  '5733bf84d058e614000b61c1',\n",
       "  '5733bed24776f41900661188',\n",
       "  '5733bed24776f41900661189',\n",
       "  '5733bed24776f4190066118a',\n",
       "  '5733bed24776f4190066118b',\n",
       "  '5733bed24776f4190066118c',\n",
       "  '5733a6424776f41900660f51'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = SquadDataset(train_df, 16)\n",
    "# train_dataset.data[:2]\n",
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "695"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('architecturally, the school has a catholic character. atop the main building\\'s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \"venite ad me omnes\". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive (and in a direct line that connects through 3 statues and the gold dome), is a simple, modern stone statue of mary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = SquadDataset(valid_df, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[200, 128, 173,  ..., 178, 178,  27],\n",
      "         [ 18,   1,   1,  ...,   1,   1,   1],\n",
      "         [ 59,  71,  39,  ...,   1,   1,   1],\n",
      "         ...,\n",
      "         [193, 141,   1,  ...,   1,   1,   1],\n",
      "         [  4, 200, 128,  ...,   1,   1,   1],\n",
      "         [ 96,   1,   1,  ...,   1,   1,   1]],\n",
      "\n",
      "        [[200, 128, 173,  ..., 178, 178,  27],\n",
      "         [ 18,   1,   1,  ...,   1,   1,   1],\n",
      "         [ 59,  71,  39,  ...,   1,   1,   1],\n",
      "         ...,\n",
      "         [193, 141,   1,  ...,   1,   1,   1],\n",
      "         [  4, 200, 128,  ...,   1,   1,   1],\n",
      "         [ 96,   1,   1,  ...,   1,   1,   1]]])\n",
      "tensor([[16248,     3,     2,   133,    40,    10,   544,   801,     5,  8708,\n",
      "             2,   236,   300,    18,  1234,  5753,    12,    10,  2326,  4968,\n",
      "             4,     2,  2676,   849,     5,  2177,     6,  1201,     4,     2,\n",
      "           236,   300,     8,  4271,    32,     3,    12,    10,   987,  4968,\n",
      "             4,  1493,    23,  2131, 54455,    23,     2,  4619,    14, 54456,\n",
      "          1020,  2343, 40455,    14,     5,   686,     9,     2,   236,   300,\n",
      "            12,     2,  4540,     4,     2,  3909,  1498,     5,  2177,  1217,\n",
      "             2,  4540,    12,     2, 19572,     3,    10,  8343,   206,     4,\n",
      "          3244,     8,  6950,     5,    32,    12,    10, 10479,     4,     2,\n",
      "         19572,    31, 19573,     3,   230,    54,     2,  2676,   849, 22981,\n",
      "          1378,     9,   722, 40456, 40457,     6,  8044,     5,    31,     2,\n",
      "           171,     4,     2,   236,  1665,    22,     8,     6,    10,   882,\n",
      "           355,    20,  4111,   117,   471,  6037,     8,     2,  1234,  5753,\n",
      "            21,     3,    12,    10,  1619,     3,   173,  1559,  4968,     4,\n",
      "           849,     5],\n",
      "        [16248,     3,     2,   133,    40,    10,   544,   801,     5,  8708,\n",
      "             2,   236,   300,    18,  1234,  5753,    12,    10,  2326,  4968,\n",
      "             4,     2,  2676,   849,     5,  2177,     6,  1201,     4,     2,\n",
      "           236,   300,     8,  4271,    32,     3,    12,    10,   987,  4968,\n",
      "             4,  1493,    23,  2131, 54455,    23,     2,  4619,    14, 54456,\n",
      "          1020,  2343, 40455,    14,     5,   686,     9,     2,   236,   300,\n",
      "            12,     2,  4540,     4,     2,  3909,  1498,     5,  2177,  1217,\n",
      "             2,  4540,    12,     2, 19572,     3,    10,  8343,   206,     4,\n",
      "          3244,     8,  6950,     5,    32,    12,    10, 10479,     4,     2,\n",
      "         19572,    31, 19573,     3,   230,    54,     2,  2676,   849, 22981,\n",
      "          1378,     9,   722, 40456, 40457,     6,  8044,     5,    31,     2,\n",
      "           171,     4,     2,   236,  1665,    22,     8,     6,    10,   882,\n",
      "           355,    20,  4111,   117,   471,  6037,     8,     2,  1234,  5753,\n",
      "            21,     3,    12,    10,  1619,     3,   173,  1559,  4968,     4,\n",
      "           849,     5]]) tensor([[    9,   569,    25,     2,  2676,   849,  5986,  1082,     6,  8044,\n",
      "             6, 19573,   230,     7],\n",
      "        [   11,    12,     6,  1201,     4,     2,  1258,  1195,   236,   300,\n",
      "             7,     1,     1,     1]]) tensor([[[200, 128, 173,  ..., 178, 178,  27],\n",
      "         [ 18,   1,   1,  ...,   1,   1,   1],\n",
      "         [ 59,  71,  39,  ...,   1,   1,   1],\n",
      "         ...,\n",
      "         [193, 141,   1,  ...,   1,   1,   1],\n",
      "         [  4, 200, 128,  ...,   1,   1,   1],\n",
      "         [ 96,   1,   1,  ...,   1,   1,   1]],\n",
      "\n",
      "        [[200, 128, 173,  ..., 178, 178,  27],\n",
      "         [ 18,   1,   1,  ...,   1,   1,   1],\n",
      "         [ 59,  71,  39,  ...,   1,   1,   1],\n",
      "         ...,\n",
      "         [193, 141,   1,  ...,   1,   1,   1],\n",
      "         [  4, 200, 128,  ...,   1,   1,   1],\n",
      "         [ 96,   1,   1,  ...,   1,   1,   1]]]) tensor([[[ 59, 193,   1,   1,   1,   1,   1,   1,   1],\n",
      "         [ 20,  71, 193,   4,   1,   1,   1,   1,   1],\n",
      "         [ 19, 123,  19,   1,   1,   1,   1,   1,   1],\n",
      "         [ 59,  71,  39,   1,   1,   1,   1,   1,   1],\n",
      "         [ 55, 123, 128, 174, 123, 187,   1,   1,   1],\n",
      "         [  4, 200, 128,  27,   1,   1,   1,   1,   1],\n",
      "         [200, 178, 178,  39, 174,  39,  19, 178,  27],\n",
      "         [200, 125, 125,  39, 200, 128,   1,   1,   1],\n",
      "         [123, 187,   1,   1,   1,   1,   1,   1,   1],\n",
      "         [ 35, 179, 191, 179,   1,   1,   1,   1,   1],\n",
      "         [123, 187,   1,   1,   1,   1,   1,   1,   1],\n",
      "         [178, 193,  75, 128,  19,  39,  64,   1,   1],\n",
      "         [141, 128, 200, 187, 173,  39,   1,   1,   1],\n",
      "         [148,   1,   1,   1,   1,   1,   1,   1,   1]],\n",
      "\n",
      "        [[ 20,  71, 200,  59,   1,   1,   1,   1,   1],\n",
      "         [123,  64,   1,   1,   1,   1,   1,   1,   1],\n",
      "         [123, 187,   1,   1,   1,   1,   1,   1,   1],\n",
      "         [141, 128, 193, 187,  59,   1,   1,   1,   1],\n",
      "         [193, 141,   1,   1,   1,   1,   1,   1,   1],\n",
      "         [ 59,  71,  39,   1,   1,   1,   1,   1,   1],\n",
      "         [187, 193,  59, 128,  39,   1,   1,   1,   1],\n",
      "         [ 19, 200,   4,  39,   1,   1,   1,   1,   1],\n",
      "         [  4, 200, 123, 187,   1,   1,   1,   1,   1],\n",
      "         [  5,  75, 123, 178,  19, 123, 187, 174,   1],\n",
      "         [148,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "         [  1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "         [  1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
      "         [  1,   1,   1,   1,   1,   1,   1,   1,   1]]]) tensor([[102, 104],\n",
      "        [ 37,  41]]) ['architecturally, the school has a catholic character. atop the main building\\'s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \"venite ad me omnes\". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive (and in a direct line that connects through 3 statues and the gold dome), is a simple, modern stone statue of mary.', 'architecturally, the school has a catholic character. atop the main building\\'s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \"venite ad me omnes\". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive (and in a direct line that connects through 3 statues and the gold dome), is a simple, modern stone statue of mary.'] ['saint bernadette soubirous', 'a copper statue of christ'] ['5733be284776f41900661182', '5733be284776f4190066117f']\n"
     ]
    }
   ],
   "source": [
    "a = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiDAF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "> *Word embedding layer also maps each word to a high-dimensional vector space. We use pre-trained word vectors, GloVe to obtain the ﬁxed word embedding of each word. *\n",
    "\n",
    "This model uses 100-dimensional pre-trained word vectors. The `weights_matrix` obtained below is initialized as an `nn.Embedding`'s weight. This is done in the last module in a function which as follows:\n",
    "\n",
    "```\n",
    "weights_matrix = np.load('bidafglove.npy')\n",
    "num_embeddings, embedding_dim = weights_matrix.shape\n",
    "embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix).to(self.device),freeze=True)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_dict():\n",
    "    '''\n",
    "    Parses the glove word vectors text file and returns a dictionary with the words as\n",
    "    keys and their respective pretrained word vectors as values.\n",
    "\n",
    "    '''\n",
    "    glove_dict = {}\n",
    "    with open(\"./data/Glove/glove.6B.100d.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            glove_dict[word] = vector\n",
    "            \n",
    "    f.close()\n",
    "    \n",
    "    return glove_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict = get_glove_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weights_matrix(glove_dict):\n",
    "    '''\n",
    "    Creates a weight matrix of the words that are common in the GloVe vocab and\n",
    "    the dataset's vocab. Initializes OOV words with a zero vector.\n",
    "    '''\n",
    "    weights_matrix = np.zeros((len(word_vocab), 100))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(word_vocab):\n",
    "        try:\n",
    "            weights_matrix[i] = glove_dict[word]\n",
    "            words_found += 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return weights_matrix, words_found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found in the GloVe vocab:  73003\n"
     ]
    }
   ],
   "source": [
    "weights_matrix, words_found = create_weights_matrix(glove_dict)\n",
    "print(\"Words found in the GloVe vocab: \" ,words_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the weights to load in future\n",
    "\n",
    "np.save('bidafglove_tv.npy', weights_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Embedding\n",
    "\n",
    "A character embedding is calculated for each context and query word. This is done by using convolutions.   \n",
    ">  *It maps each word to a vector space using character-level CNNs.*\n",
    "\n",
    "Using CNNs in NLP was first proposed by Yoon Kim in 2014 in his paper \"Convolutional Neural Networks for Sentence Classification\". This paper tries to use CNNs in NLP as they are used in vision. Most of the state-of-the-art results in CV at that time were achieved by transfer learning from larger models pretrained on ImageNet. In this paper, they train a simple CNN with one layer of convolution on top of pretrained word vectors and hypothesized that these pretrained word vectors could work as a universal feature extractors for various classification tasks. This is analogous to the earlier layers of vision models like VGG and Inception working as generic feature extractors.\n",
    "\n",
    "The intuition is simple over here. Just as convolutional filters learn various features in an image by operating on its pixels, here they'll do so by operating on characters of words. Let's get into the working of this layer.  \n",
    "\n",
    "We first pass each word through an embedding layer to get a fixed size vector. Let the embedding dimension be $d$.\n",
    "Let $C$ represent a matrix representation of word of length $l$. Therefore $C$ is a matrix with dimensions $d$ x $l$.\n",
    "<img src=\"images/charemb1.PNG\" width=\"500\" height=\"300\"/>\n",
    "\n",
    "Let $H$ represent a convolutional filter with dimensions $d$ x $w$, where $d$ is the embedding dimension and $w$ is the width or the window size of the filter.   \n",
    "The weights of this filter are randomly initialized and learnt parallelly via backpropogation. We convolve this filter $H$ over our word representation $C$ as shown below. \n",
    "<img src=\"images/charemb2.PNG\" width=\"500\" height=\"400\"/>  \n",
    "\n",
    "The convolution operation is simply the inner product of the filter $H$ and matrix $C$. The convolution operations can be visualized as follows:\n",
    "<img src=\"images/charemb3.PNG\" width=\"900\" height=\"700\"/>    \n",
    "The result of the above operation is a feature vector. A single filter is usually associated with a unique feature that it captures from the image/matrix. To get the most representative value related to the feature, we perform max pooling over the dimension of this vector.\n",
    "<img src=\"images/charemb4.PNG\" width=\"500\" height=\"300\"/>     \n",
    "\n",
    "The above process was described for a single filter. This same process is repeated with $N$ number of filters. Each of these filters captures a different property of word. In an image, for example, if one filter captures the edges, another filter will capture the texture and another one the shapes in the image and so on. $N$ is also the size of the desired character embedding. In this paper authors have trained the model with $N$ = 100.  \n",
    "\n",
    "The implementation of this layer is fairly straightforward.  The input to this layer is of dimension `[batch_size, seq_len, word_len]` where `seq_len` and `word_len` are the lengths of largest sequence and word respectively within a given batch . We first embed the character tokens into a fixed size vector using an embedding layer. This gives a vector of dimension `[batch_size, seq_len, word_len, emb_dim]`.   \n",
    "We then convert this tensor into a format that closely resembles an image, of type [ $N$, $C_{in}$, $H_{in}$, $W_{in}$]. The number of input channels, $C_{in}$ would be 1 and the output channels would be the desired embedding size which is 100. This is then passed through the convolution layer which gives an output of shape [ $N$, $C_{out}$, $H_{out}$, $W_{out}$]. Here,\n",
    "\n",
    "<img src=\"images/conv.PNG\" width=\"600\" height=\"500\"/>  \n",
    "\n",
    "If `padding` = [0,0], `kernel_size` (or filter_size) = [$H_{in}$, $w$], `dilation` = [1,1], `stride` = [1,1] (as visible in images above), then,   \n",
    "$H_{out}$ = 1, and $W_{out}$ = $W_{in}$ - $w$ - 1.\n",
    "\n",
    "Since $H_{out}$ = 1, we squeeze that dimension and perform max pooling with a kernel_size = $L_{in}$. The value of $L_{in}$ =  $W_{in}$ - $w$ - 1.\n",
    "\n",
    "<img src=\"images/maxpool.PNG\" width=\"600\" height=\"500\"/>     \n",
    "If the kernel size = $L_{in}$, we get $L_{out}$ = 1 if other values are default. This dimension is again squeezed to finally give us a tensor of dimension   `[batch_size, seq_len, output_channels (or 100)]`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterEmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, char_vocab_dim, char_emb_dim, num_output_channels, kernel_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.char_emb_dim = char_emb_dim\n",
    "        \n",
    "        self.char_embedding = nn.Embedding(char_vocab_dim, char_emb_dim, padding_idx=1)\n",
    "        \n",
    "        self.char_convolution = nn.Conv2d(in_channels=1, out_channels=100, kernel_size=kernel_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = [bs, seq_len, word_len]\n",
    "        # returns : [batch_size, seq_len, num_output_channels]\n",
    "        # the output can be thought of as another feature embedding of dim 100.\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = self.dropout(self.char_embedding(x))\n",
    "        # x = [bs, seq_len, word_len, char_emb_dim]\n",
    "        \n",
    "        # following three operations manipulate x in such a way that\n",
    "        # it closely resembles an image. this format is important before \n",
    "        # we perform convolution on the character embeddings.\n",
    "        \n",
    "        x = x.permute(0,1,3,2)\n",
    "        # x = [bs, seq_len, char_emb_dim, word_len]\n",
    "        \n",
    "        x = x.view(-1, self.char_emb_dim, x.shape[3])\n",
    "        # x = [bs*seq_len, char_emb_dim, word_len]\n",
    "        \n",
    "        x = x.unsqueeze(1)\n",
    "        # x = [bs*seq_len, 1, char_emb_dim, word_len]\n",
    "        \n",
    "        # x is now in a format that can be accepted by a conv layer. \n",
    "        # think of the tensor above in terms of an image of dimension\n",
    "        # (N, C_in, H_in, W_in).\n",
    "        \n",
    "        x = self.relu(self.char_convolution(x))\n",
    "        # x = [bs*seq_len, out_channels, H_out, W_out]\n",
    "        \n",
    "        x = x.squeeze()\n",
    "        # x = [bs*seq_len, out_channels, W_out]\n",
    "                \n",
    "        x = F.max_pool1d(x, x.shape[2]).squeeze()\n",
    "        # x = [bs*seq_len, out_channels, 1] => [bs*seq_len, out_channels]\n",
    "        \n",
    "        x = x.view(batch_size, -1, x.shape[-1])\n",
    "        # x = [bs, seq_len, out_channels]\n",
    "        # x = [bs, seq_len, features] = [bs, seq_len, 100]\n",
    "        \n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highway Networks\n",
    "\n",
    "Highway networks were originally introduced to ease the training of deep neural networks. While researchers had cracked the code for optimizing shallow neural networks, training *deep* networks was still a challenging task owing to problems such as vanishing gradients etc. Quoting the paper,\n",
    "\n",
    ">  *We present a novel architecture that enables the optimization of networks with virtually arbitrary depth. This is accomplished through the use of a learned gating mechanism for regulating information ﬂow which is inspired by Long Short Term Memory recurrent neural networks. Due to this gating mechanism, a neural network can have paths along which information can ﬂow across several layers without attenuation. We call such paths information highways, and such networks highway networks.* \n",
    "\n",
    "This paper takes the key idea of learned gating mechanism from LSTMs which process information internally through a sequence of learned gates. The purpose of this layer is to *learn* to pass relevant information from the input. A highway network is a series of feed-forward or linear layers with a gating mechanism. The gating is implemented by using a sigmoid function which decides what amount of information should be transformed and what should be passed as it is.   \n",
    "\n",
    "A plain feed-forward layer is associated with a linear transform $H$ parameterized by ($W_{H}, b_{H}$), such that for input $x$, the output $y$ is  \n",
    "\n",
    "$$ y = g(W_{H}.x + b_{H})$$\n",
    "where $g$ is a non-linear activation.  \n",
    "For highway networks, two additional linear transforms are defined viz. $T$ ($W_{T},b_{T}$) and $C$ ($W_{C}$,$b_{C}$).\n",
    "Then,    \n",
    "  \n",
    "$$ y = T(x) . H(x) + x . C(x) $$ \n",
    "> *We refer to T as the transform gate and C as the carry gate, since they express how much of the output is produced by\n",
    "transforming the input and carrying it, respectively. For simplicity, in this paper we set C = 1 − T. *\n",
    "\n",
    "$$ y = T(x) . H(x) + x . (1 - T(x)) $$  \n",
    "  \n",
    "$$ y = T(x) . g(W_{H}.x + b_{H}) + x . (1 - T(x)) $$  \n",
    "where $T(x)$ = $\\sigma$ ($W_{T}$ . $x$ + $b_{T}$) and $g$ is relu activation.  \n",
    "\n",
    "The input to this layer is the concatenation of word and character embeddings of each word. To implement this we use `nn.ModuleList` to add multiple linear layers. This is done for the gate layer as well as for a normal linear transform. In code the `flow_layer` is the same as linear transform $H$ discussed above and `gate_layer` is $T$. In the forward method we loop through each layer and compute the output according to the highway equation described above.   \n",
    "  \n",
    "The output of this layer for context is $X$ $\\epsilon$ $R^{\\ d \\ X \\ T}$ and for query is $Q$ $\\epsilon$ $R^{\\ d \\ X \\ J}$, where $d$ is hidden size of the LSTM, $T$ is the context length, $J$ is the query length.  \n",
    "\n",
    "The structure discussed so far is a recurring pattern in many NLP systems. Although this might be out of favor now with the advent of transformers and large pretrained language models, you will find this pattern in many NLP systems before transformers came into being. The idea behind this is that adding highway layers enables the network to make more efficient use of character embeddings. If a particular word is not found in the pretrained word vector vocabulary (OOV word), it will most likely be initialized with a zero vector. It then makes much more sense to look at the character embedding of that word rather than the word embedding. The soft gating mechanism in highway layers helps the model to achieve this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, num_layers=2):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.flow_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)])\n",
    "        self.gate_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            flow_value = F.relu(self.flow_layer[i](x))\n",
    "            gate_value = torch.sigmoid(self.gate_layer[i](x))\n",
    "            \n",
    "            x = gate_value * flow_value + (1-gate_value) * x\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Embedding\n",
    "\n",
    "This layer is the final embedding layer in the model.\n",
    "> *Bi-Directional Attention Flow (BIDAF) network, a hierarchical multi-stage architecture for modeling the representations ofthe context paragraph at different levels of granularity. BIDAF includes character-level, word-level, and contextual embeddings*\n",
    "\n",
    "The output of highway layers is passed to a bidirection LSTM to model the temporal features of the text. This is done for both, the context and the query.   \n",
    ">  *Utilizes contextual cues from surrounding words to reﬁne the embedding of the words*\n",
    "\n",
    "The output of this layer for context is called as $H$ $\\epsilon$ $R^{\\ 2d \\ X \\ T}$ and for query it is named as $U$ $\\epsilon$ $R^{\\ 2d \\  X \\  J}$. The $2d$ is because of the features from both forward and backward LSTMs. In code, we simply use the outputs of the LSTM layer and ignore the hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualEmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.highway_net = HighwayNetwork(input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = [bs, seq_len, input_dim] = [bs, seq_len, emb_dim*2]\n",
    "        # the input is the concatenation of word and characeter embeddings\n",
    "        # for the sequence.\n",
    "        \n",
    "        highway_out = self.highway_net(x)\n",
    "        # highway_out = [bs, seq_len, input_dim]\n",
    "        \n",
    "        outputs, _ = self.lstm(highway_out)\n",
    "        # outputs = [bs, seq_len, emb_dim*2]\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Flow Layer\n",
    "\n",
    "\n",
    "> *It is worth noting that the ﬁrst three layers of the model are computing features from the query and context at different levels of granularity, akin to the multi-stage feature computation of convolutional neural networks in the computer vision ﬁeld.*\n",
    "\n",
    "The output of the previous layer: contextual representation of context $H$ and query $U$ are passed on to this layer. Until now processing of the context and the query has been independent of each other. This layer, however, is responsible for fusing and linking the context and query representations.   \n",
    "\n",
    "This layer calculates attention in two directions: from context to query and from query to context. Attention vectors for these calculations are derived from a common matrix which is called as the similarity matrix and is denoted by $S$ $\\epsilon$ $R^{\\ T \\ X \\ J}$. $T$ is the length of the context (number of words/tokens) and $J$ is the length of the query for each training example. The similarity matrix is computed by,\n",
    "$$ S_{tj} = \\alpha\\ (H_{:t}, U_{:j}) $$  \n",
    "\n",
    "where $S_{tj}$ $\\epsilon$ $R$.  \n",
    "$S_{tj}$ is a single float value that determines the similarity between the $t$-th context word and $j$-th query word.\n",
    "$\\alpha$ is a trainable function that encodes the similarity between two input vectors $H_{:t}$ and $U_{:j}$.  \n",
    "$H_{:t}$ is the contextual representation of the $t$-th context word and $U_{:j}$ is the contextual representation of the $j$-th query word. \n",
    "<img src=\"images/simimat.PNG\" width=\"500\" height=\"500\"/>     \n",
    "\n",
    "\n",
    "\n",
    "The trainable function is defined as,  \n",
    "\n",
    "$$ \\alpha \\ (h, u) = w_{(S)}^{T}\\ [h\\ ;\\ u \\ ; \\ h \\circ u]  $$\n",
    "where $;$ denotes concatenation and $\\circ$ denotes an element wise product.    \n",
    "$w_{T}^{S}$ is a trainable weight matrix of dimension $6d$. This is because $H$ $\\epsilon$ $R^{\\ 2d \\ X \\ T}$ and $U$ $\\epsilon$ $R^{\\ 2d \\ X \\ J}$, and we are concatenating 3 such vectors.  \n",
    "\n",
    "In code, all the computations above are performed directly by operating on matrices/ tensors. We first `repeat` the contextual representations of context and query along appropriate dimensions to get two tensors of shape `[batch_size, ctx_len, query_len, emb_dim*2 (d*2)]`. $w_{(S)}^T$ is characterized by a linear layer called `similarity_weight`. We concat the tensors along the last dimension and pass it through the linear layer. \n",
    "\n",
    "### Context-to-Query Attention\n",
    "\n",
    "> *. Context-to-query (C2Q) attention signiﬁes which query words are most relevant to each context word.*\n",
    " \n",
    "Let $a_{t}$ represent the vector that encodes the attention paid on each query word by the $t$-th context word.   \n",
    "$$ \\sum_{j}a_{tj} = 1  $$  \n",
    "$$ a_{t} = softmax(S_{:t}) $$\n",
    "where $a_{t}$ $\\epsilon$ $R^{J}$   \n",
    "This can be visualized as,\n",
    "<img src=\"images/c2q.PNG\" width=\"700\" height=\"750\"/>     \n",
    "\n",
    "Subsequently, each attended query vector is calculated by,\n",
    "$$ \\overline U_{:t} = \\sum_{j} a_{tj} U_{:j} $$\n",
    "\n",
    "This vector indicates the most important word in the query with respect to context.  \n",
    "\n",
    "In code, again vectorizing operations into tensors, this simply involves taking a softmax of the similarity matrix across the last dimension, i.e across the columns and multiplying this with $U$. The shape of similarity matrix is `[batch_size, ctx_len, query_len]`. The shape of $U$ is `[batch_size, query_len, emb_dim*2]` . Hence performing matrix multiplication by using `torch.bmm` would give us a tensor of shape `[batch_size, ctx_len, emb_dim*2 ]`. Therefore, $\\overline U$ will be $2d$ X $T$\n",
    " matrix.\n",
    "\n",
    "\n",
    "### Query-to-Context Attention\n",
    "\n",
    ">  *Query-to-context (Q2C) attention signiﬁes which context words have the closest similarity to one of the query words and are hence critical for answering the query.*\n",
    "\n",
    "The attention vector here is calculated as,  \n",
    "$$ b = softmax \\ (max_{col} \\ (S)) \\ \\epsilon R^{T} $$  \n",
    "where $max_{col}$ performs the maximum function across the column. The attended context vector is then calculated as\n",
    "\n",
    "$$ \\overline h = \\sum_{t} b_{t} H_{:t}$$\n",
    "\n",
    "The above equations can be visualized as,\n",
    "\n",
    "<img src=\"images/q2c.PNG\" width=\"800\" height=\"900\"/>\n",
    "\n",
    "The above figure helps us in understanding that for each training example we'll get a single $T$ - dimensional vector. This vector will then be multiplied by the contextual representation $H$ to get a single $2d$ vector.  To get the matrix $\\overline H$, we need to tile/repeat this vector $T$ times to get a $2d$-by-$T$ representation.\n",
    "This is unlike the previous attention we calculated where we directly got a $2d$-by-$T$ matrix.\n",
    "\n",
    "The implementation for this part is straightforward and very similar to the explanation above. We first perform maximum across columns using `torch.max`. This gives a tensor of shape `[batch_size, ctx_len]`. This is then passed through a softmax to get weights that add up to 1. We then insert an extra dimension in this tensor and multiply it with $H$. This gives a tensor of shape `[batch_size, 1, emb_dim*2]`. This is exactly what we have discussed above. This tensor corresponds to $\\overline h$. We then `repeat` this $T$ times to get $\\overline H$ of shape `[batch_size, ctx_len, emb_dim*2]`\n",
    "\n",
    "\n",
    "### Combining attention vectors and contextual embeddings\n",
    "\n",
    "> *Finally, the contextual embeddings and the attention vectors are combined together to yield G, where each column vector can be considered as the query-aware representation of each context word.*\n",
    "\n",
    "$G$ is defined as,\n",
    "\n",
    "$$ G_{:t} = \\beta \\ (\\ H_{:t}\\ ;\\ \\overline U_{:t} \\ ;\\ \\overline H_{:t}  \\ )$$\n",
    "\n",
    "where $\\beta$ is a trainable function.  \n",
    "$$ \\beta (h, \\overline u, \\overline h) = [h\\ ;\\ \\overline u\\ ;\\ h \\circ \\overline u\\ ;\\ h \\circ \\overline h\\ ] $$\n",
    "\n",
    "This concatenation gives a $8d$ vector. Hence the trainable weight here should have an input dimension of $8d$. However here we don't involve a trainable weight because according to the authors,\n",
    "\n",
    "> *While the β function can be an arbitrary trainable neural network, such as multi-layer perceptron, a simple concatenation as following still shows good performance in our experiments.*\n",
    "\n",
    "In code, this simply involves a single line of concatenating tensors using `torch.cat` to yeild $G$. This tensor holds the query-aware representation of the context words.\n",
    "\n",
    "\n",
    "## Modeling Layer\n",
    "\n",
    "$G$ is then passed on to this layer. This layer is responsible for capturing temporal features interactions among the context words. This is done using a bidirectional LSTM. The difference between this layer and the contextual layer, both of which involve an LSTM layer is that here we have a *query-aware* representation of the context while in the contextual layer, encoding of the context and query was independent.  \n",
    "Using an LSTM layer with a hidden size of $d$ we get a $2d$-by-$T$ output called $M$.  \n",
    "\n",
    "> *Each column vector of M is expected to contain contextual information about the word with respect to the entire context paragraph and the query.*\n",
    "\n",
    "\n",
    "## Output Layer\n",
    "\n",
    "The start index $p^{1}$ of the answer span is calculated by,\n",
    "\n",
    "<img src=\"images/p1.PNG\" width=\"300\" height=\"300\"/>\n",
    "\n",
    "where $w_{(p^{1})}$ is a $10d$ trainable weight vector which corresponds to a linear layer in code called `output_start`.  \n",
    "To predict the end of the answer span, $M$ is once again passed to a bidirectional LSTM to get $M^{2}$ which again is a $2d$-by-$T$ matrix. The end is then computed as,  \n",
    "\n",
    "<img src=\"images/p2.PNG\" width=\"300\" height=\"300\"/>  \n",
    "\n",
    "\n",
    "$w_{(p^{2})}$ is again a $10d$ trainable weight vector and corresponds to the linear layer `output_end`. \n",
    "\n",
    "In code we don't explicitly calculate softmax since this is taken care of while calculating the losses during training. \n",
    "\n",
    "> *The output layer is application-speciﬁc. The modular nature of BIDAF allows us to easily swap out the output layer based on the task, with the rest of the architecture remaining exactly the same. *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiDAF(nn.Module):\n",
    "    \n",
    "    def __init__(self, char_vocab_dim, emb_dim, char_emb_dim, num_output_channels, \n",
    "                 kernel_size, ctx_hidden_dim, device):\n",
    "        '''\n",
    "        char_vocab_dim = len(char2idx)\n",
    "        emb_dim = 100\n",
    "        char_emb_dim = 8\n",
    "        num_output_chanels = 100\n",
    "        kernel_size = (8,5)\n",
    "        ctx_hidden_dim = 100\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.word_embedding = self.get_glove_embedding()\n",
    "        \n",
    "        self.character_embedding = CharacterEmbeddingLayer(char_vocab_dim, char_emb_dim, \n",
    "                                                      num_output_channels, kernel_size)\n",
    "        \n",
    "        self.contextual_embedding = ContextualEmbeddingLayer(emb_dim*2, ctx_hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        self.similarity_weight = nn.Linear(emb_dim*6, 1, bias=False)\n",
    "        \n",
    "        self.modeling_lstm = nn.LSTM(emb_dim*8, emb_dim, bidirectional=True, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        self.output_start = nn.Linear(emb_dim*10, 1, bias=False)\n",
    "        \n",
    "        self.output_end = nn.Linear(emb_dim*10, 1, bias=False)\n",
    "        \n",
    "        self.end_lstm = nn.LSTM(emb_dim*2, emb_dim, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    \n",
    "    def get_glove_embedding(self):\n",
    "        \n",
    "        weights_matrix = np.load('bidafglove_tv.npy')\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape\n",
    "        embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix).to(self.device),freeze=True)\n",
    "\n",
    "        return embedding\n",
    "        \n",
    "    def forward(self, ctx, ques, char_ctx, char_ques):\n",
    "        # ctx = [bs, ctx_len]\n",
    "        # ques = [bs, ques_len]\n",
    "        # char_ctx = [bs, ctx_len, ctx_word_len]\n",
    "        # char_ques = [bs, ques_len, ques_word_len]\n",
    "        \n",
    "        ctx_len = ctx.shape[1]\n",
    "        \n",
    "        ques_len = ques.shape[1]\n",
    "        \n",
    "        ## GET WORD AND CHARACTER EMBEDDINGS\n",
    "        \n",
    "        ctx_word_embed = self.word_embedding(ctx)\n",
    "        # ctx_word_embed = [bs, ctx_len, emb_dim]\n",
    "        \n",
    "        ques_word_embed = self.word_embedding(ques)\n",
    "        # ques_word_embed = [bs, ques_len, emb_dim]\n",
    "        \n",
    "#         ctx_char_embed = self.character_embedding(char_ctx)\n",
    "        # ctx_char_embed =  [bs, ctx_len, emb_dim]\n",
    "        \n",
    "#         ques_char_embed = self.character_embedding(char_ques)\n",
    "        # ques_char_embed = [bs, ques_len, emb_dim]\n",
    "        \n",
    "        ## CREATE CONTEXTUAL EMBEDDING\n",
    "        \n",
    "#         ctx_contextual_inp = torch.cat([ctx_word_embed, ctx_char_embed],dim=2)\n",
    "        ctx_contextual_inp = torch.cat([ctx_word_embed, ctx_word_embed],dim=2)\n",
    "        # [bs, ctx_len, emb_dim*2]\n",
    "        \n",
    "#         ques_contextual_inp = torch.cat([ques_word_embed, ques_char_embed],dim=2)\n",
    "        ques_contextual_inp = torch.cat([ques_word_embed, ques_word_embed],dim=2)\n",
    "        # [bs, ques_len, emb_dim*2]\n",
    "        \n",
    "        ctx_contextual_emb = self.contextual_embedding(ctx_contextual_inp)\n",
    "        # [bs, ctx_len, emb_dim*2]\n",
    "        \n",
    "        ques_contextual_emb = self.contextual_embedding(ques_contextual_inp)\n",
    "        # [bs, ques_len, emb_dim*2]\n",
    "        \n",
    "        \n",
    "        ## CREATE SIMILARITY MATRIX\n",
    "        \n",
    "        ctx_ = ctx_contextual_emb.unsqueeze(2).repeat(1,1,ques_len,1)\n",
    "        # [bs, ctx_len, 1, emb_dim*2] => [bs, ctx_len, ques_len, emb_dim*2]\n",
    "        \n",
    "        ques_ = ques_contextual_emb.unsqueeze(1).repeat(1,ctx_len,1,1)\n",
    "        # [bs, 1, ques_len, emb_dim*2] => [bs, ctx_len, ques_len, emb_dim*2]\n",
    "        \n",
    "        elementwise_prod = torch.mul(ctx_, ques_)\n",
    "        # [bs, ctx_len, ques_len, emb_dim*2]\n",
    "        \n",
    "        alpha = torch.cat([ctx_, ques_, elementwise_prod], dim=3)\n",
    "        # [bs, ctx_len, ques_len, emb_dim*6]\n",
    "        \n",
    "        similarity_matrix = self.similarity_weight(alpha).view(-1, ctx_len, ques_len)\n",
    "        # [bs, ctx_len, ques_len]\n",
    "        \n",
    "        \n",
    "        ## CALCULATE CONTEXT2QUERY ATTENTION\n",
    "        \n",
    "        a = F.softmax(similarity_matrix, dim=-1)\n",
    "        # [bs, ctx_len, ques_len]\n",
    "        \n",
    "        c2q = torch.bmm(a, ques_contextual_emb)\n",
    "        # [bs] ([ctx_len, ques_len] X [ques_len, emb_dim*2]) => [bs, ctx_len, emb_dim*2]\n",
    "        \n",
    "        \n",
    "        ## CALCULATE QUERY2CONTEXT ATTENTION\n",
    "        \n",
    "        b = F.softmax(torch.max(similarity_matrix,2)[0], dim=-1)\n",
    "        # [bs, ctx_len]\n",
    "        \n",
    "        b = b.unsqueeze(1)\n",
    "        # [bs, 1, ctx_len]\n",
    "        \n",
    "        q2c = torch.bmm(b, ctx_contextual_emb)\n",
    "        # [bs] ([bs, 1, ctx_len] X [bs, ctx_len, emb_dim*2]) => [bs, 1, emb_dim*2]\n",
    "        \n",
    "        q2c = q2c.repeat(1, ctx_len, 1)\n",
    "        # [bs, ctx_len, emb_dim*2]\n",
    "        \n",
    "        ## QUERY AWARE REPRESENTATION\n",
    "        \n",
    "        G = torch.cat([ctx_contextual_emb, c2q, \n",
    "                       torch.mul(ctx_contextual_emb,c2q), \n",
    "                       torch.mul(ctx_contextual_emb, q2c)], dim=2)\n",
    "        \n",
    "        # [bs, ctx_len, emb_dim*8]\n",
    "        \n",
    "        \n",
    "        ## MODELING LAYER\n",
    "        \n",
    "        M, _ = self.modeling_lstm(G)\n",
    "        # [bs, ctx_len, emb_dim*2]\n",
    "        \n",
    "        ## OUTPUT LAYER\n",
    "        \n",
    "        M2, _ = self.end_lstm(M)\n",
    "        \n",
    "        # START PREDICTION\n",
    "        \n",
    "        p1 = self.output_start(torch.cat([G,M], dim=2))\n",
    "        # [bs, ctx_len, 1]\n",
    "        \n",
    "        p1 = p1.squeeze()\n",
    "        # [bs, ctx_len]\n",
    "        \n",
    "        #p1 = F.softmax(p1, dim=-1)\n",
    "        \n",
    "        # END PREDICTION\n",
    "        \n",
    "        p2 = self.output_end(torch.cat([G, M2], dim=2)).squeeze()\n",
    "        # [bs, ctx_len, 1] => [bs, ctx_len]\n",
    "        \n",
    "        #p2 = F.softmax(p2, dim=-1)\n",
    "        \n",
    "        \n",
    "        return p1, p2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    ">  *We use 100 1D ﬁlters for CNN char embedding, each with a width of 5. The hidden state size (d) of the model is 100. The model has about 2.6 million parameters. We use the AdaDelta(Zeiler,2012) optimizer, with a mini batch size of 60 and an initial learning rate of 0.5, for 12 epochs. A dropout rate of 0.2 isused for the CNN, all LSTM layers, and the linear transformation before the softmax for the answers.*\n",
    "\n",
    "__Note__- Although the mini-batch size mentioned here is 60, you might need to change this depending on your GPU. The authors have trained this model on Titan X. I have used GTX 1080 Ti for training this model which has a RAM of 11.2 GB. I had to reduce my mini-batch size to 16 or 12 to make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_VOCAB_DIM = len(char2idx)\n",
    "EMB_DIM = 100\n",
    "CHAR_EMB_DIM = 8\n",
    "NUM_OUTPUT_CHANNELS = 100\n",
    "KERNEL_SIZE = (8,5)\n",
    "HIDDEN_DIM = 100\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = BiDAF(CHAR_VOCAB_DIM, \n",
    "              EMB_DIM, \n",
    "              CHAR_EMB_DIM, \n",
    "              NUM_OUTPUT_CHANNELS, \n",
    "              KERNEL_SIZE, \n",
    "              HIDDEN_DIM, \n",
    "              device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "optimizer = optim.Adadelta(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset):\n",
    "    print(\"Starting training ........\")\n",
    "   \n",
    "\n",
    "    train_loss = 0.\n",
    "    batch_count = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataset):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "#         if batch_count % 10 == 0:\n",
    "#             break\n",
    "            \n",
    "        if batch_count % 500 == 0:\n",
    "            print(f\"Starting batch: {batch_count}\")\n",
    "        batch_count += 1\n",
    "        \n",
    "        context, question, char_ctx, char_ques, label, ctx_text, ans, ids = batch\n",
    "\n",
    "        context, question, char_ctx, char_ques, label = context.to(device), question.to(device),\\\n",
    "                                   char_ctx.to(device), char_ques.to(device), label.to(device)\n",
    "\n",
    "\n",
    "        preds = model(context, question, char_ctx, char_ques)\n",
    "\n",
    "        start_pred, end_pred = preds\n",
    "\n",
    "        s_idx, e_idx = label[:,0], label[:,1]\n",
    "\n",
    "        loss = F.cross_entropy(start_pred, s_idx) + F.cross_entropy(end_pred, e_idx)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "#         plot_grad_flow(model.named_parameters())\n",
    "        \n",
    "#         for name, param in model.named_parameters():\n",
    "#             if(param.requires_grad) and (\"bias\" not in name):\n",
    "#                 writer.add_histogram(name+'_grad',param.grad.abs().mean())\n",
    "    \n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss/len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, valid_dataset):\n",
    "    \n",
    "    print(\"Starting validation .........\")\n",
    "   \n",
    "    valid_loss = 0.\n",
    "\n",
    "    batch_count = 0\n",
    "    \n",
    "    f1, em = 0., 0.\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "   \n",
    "    predictions = {}\n",
    "    \n",
    "    for batch in valid_dataset:\n",
    "\n",
    "        \n",
    "#         if batch_count % 10 == 0:\n",
    "#             break\n",
    "        if batch_count % 500 == 0:\n",
    "            print(f\"Starting batch {batch_count}\")\n",
    "        batch_count += 1\n",
    "\n",
    "        context, question, char_ctx, char_ques, label, ctx, answers, ids = batch\n",
    "\n",
    "        context, question, char_ctx, char_ques, label = context.to(device), question.to(device),\\\n",
    "                                   char_ctx.to(device), char_ques.to(device), label.to(device)\n",
    "        \n",
    "       \n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            s_idx, e_idx = label[:,0], label[:,1]\n",
    "\n",
    "            preds = model(context, question, char_ctx, char_ques)\n",
    "\n",
    "            p1, p2 = preds\n",
    "\n",
    "            \n",
    "            loss = F.cross_entropy(p1, s_idx) + F.cross_entropy(p2, e_idx)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            batch_size, c_len = p1.size()\n",
    "            ls = nn.LogSoftmax(dim=1)\n",
    "            mask = (torch.ones(c_len, c_len) * float('-inf')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\n",
    "            score, s_idx = score.max(dim=1)\n",
    "            score, e_idx = score.max(dim=1)\n",
    "            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n",
    "            \n",
    "           \n",
    "            for i in range(batch_size):\n",
    "                id = ids[i]\n",
    "                pred = context[i][s_idx[i]:e_idx[i]+1]\n",
    "                pred = ' '.join([idx2word[idx.item()] for idx in pred])\n",
    "                predictions[id] = pred\n",
    "            \n",
    "\n",
    "    \n",
    "    em, f1 = evaluate(predictions)\n",
    "    return valid_loss/len(valid_dataset), em, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions):\n",
    "    '''\n",
    "    Gets a dictionary of predictions with question_id as key\n",
    "    and prediction as value. The validation dataset has multiple \n",
    "    answers for a single question. Hence we compare our prediction\n",
    "    with all the answers and choose the one that gives us\n",
    "    the maximum metric (em or f1). \n",
    "    This method first parses the JSON file, gets all the answers\n",
    "    for a given id and then passes the list of answers and the \n",
    "    predictions to calculate em, f1.\n",
    "    \n",
    "    \n",
    "    :param dict predictions\n",
    "    Returns\n",
    "    : exact_match: 1 if the prediction and ground truth \n",
    "      match exactly, 0 otherwise.\n",
    "    : f1_score: \n",
    "    '''\n",
    "    with open('./data/Squad/squad_dev.json','r',encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "        \n",
    "    dataset = dataset['data']\n",
    "    f1 = exact_match = total = 0\n",
    "    for article in dataset:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                total += 1\n",
    "                if qa['id'] not in predictions:\n",
    "                    continue\n",
    "                \n",
    "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
    "                \n",
    "                prediction = predictions[qa['id']]\n",
    "                \n",
    "                exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "                \n",
    "                f1 += metric_max_over_ground_truths(\n",
    "                    f1_score, prediction, ground_truths)\n",
    "                \n",
    "    \n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "    \n",
    "    return exact_match, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    '''\n",
    "    Performs a series of cleaning steps on the ground truth and \n",
    "    predicted answer.\n",
    "    '''\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    '''\n",
    "    Returns maximum value of metrics for predicition by model against\n",
    "    multiple ground truths.\n",
    "    \n",
    "    :param func metric_fn: can be 'exact_match_score' or 'f1_score'\n",
    "    :param str prediction: predicted answer span by the model\n",
    "    :param list ground_truths: list of ground truths against which\n",
    "                               metrics are calculated. Maximum values of \n",
    "                               metrics are chosen.\n",
    "                            \n",
    "    \n",
    "    '''\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "        \n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns f1 score of two strings.\n",
    "    '''\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns exact_match_score of two strings.\n",
    "    '''\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Starting training ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                  | 0/5413 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▋                                    | 501/5413 [01:29<12:58,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████▏                               | 1001/5413 [02:57<13:11,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████▊                            | 1501/5413 [04:36<10:28,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████▍                        | 2001/5413 [06:32<11:31,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|██████████████████                     | 2501/5413 [08:20<09:50,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████▌                 | 3001/5413 [10:05<09:07,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|█████████████████████████▏             | 3501/5413 [11:53<06:38,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████▊          | 4001/5413 [13:28<05:14,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████▍      | 4501/5413 [15:14<03:12,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████████████████████████████████████   | 5001/5413 [16:56<01:33,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5413/5413 [18:22<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 4.65327239948303| Time: 23m 13s\n",
      "Epoch valid loss: 5.431483743184093\n",
      "Epoch EM: 28.997161778618732\n",
      "Epoch F1: 40.3208747093589\n",
      "====================================================================================\n",
      "Epoch 2\n",
      "Starting training ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                  | 0/5413 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▋                                    | 501/5413 [01:33<12:58,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████▏                               | 1001/5413 [03:01<13:09,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████▊                            | 1502/5413 [04:28<09:16,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████▍                        | 2001/5413 [06:17<11:45,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|██████████████████                     | 2501/5413 [08:05<09:54,  4.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████▌                 | 3001/5413 [09:49<08:54,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|█████████████████████████▏             | 3501/5413 [11:35<06:09,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████▊          | 4001/5413 [13:23<05:19,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████▍      | 4501/5413 [15:11<03:58,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████████████████████████████████████   | 5001/5413 [17:15<01:35,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5413/5413 [18:42<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 4.434699736195572| Time: 23m 29s\n",
      "Epoch valid loss: 5.400505067141198\n",
      "Epoch EM: 30.558183538315987\n",
      "Epoch F1: 42.27860882461797\n",
      "====================================================================================\n",
      "Epoch 3\n",
      "Starting training ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 1/5413 [00:00<17:09,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▋                                    | 501/5413 [01:47<16:13,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████▏                               | 1001/5413 [03:34<16:02,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████▊                            | 1502/5413 [05:07<09:20,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████▍                        | 2001/5413 [06:54<12:11,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|██████████████████                     | 2501/5413 [08:45<10:17,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████▌                 | 3001/5413 [10:30<09:00,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|█████████████████████████▏             | 3501/5413 [12:16<06:14,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████▊          | 4001/5413 [13:59<05:13,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████▍      | 4501/5413 [15:54<03:22,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████████████████████████████████████   | 5001/5413 [17:40<01:32,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5413/5413 [19:09<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 4.257521064823664| Time: 23m 56s\n",
      "Epoch valid loss: 5.381577951861407\n",
      "Epoch EM: 31.078524124881742\n",
      "Epoch F1: 42.64638078980324\n",
      "====================================================================================\n",
      "Epoch 4\n",
      "Starting training ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                  | 0/5413 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▋                                    | 501/5413 [01:33<13:31,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████▏                               | 1001/5413 [03:02<13:02,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████▊                            | 1502/5413 [04:33<08:57,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████▍                        | 2001/5413 [06:16<12:29,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|██████████████████                     | 2501/5413 [08:13<09:52,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████▌                 | 3001/5413 [09:56<08:55,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|█████████████████████████▏             | 3501/5413 [11:42<06:11,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████▊          | 4001/5413 [13:26<05:04,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████▍      | 4501/5413 [15:11<03:14,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████████████████████████████████████   | 5001/5413 [16:54<01:31,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5413/5413 [18:20<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 4.118458608900049| Time: 23m 7s\n",
      "Epoch valid loss: 5.4124905886267545\n",
      "Epoch EM: 31.163670766319772\n",
      "Epoch F1: 42.90313216454146\n",
      "====================================================================================\n",
      "Epoch 5\n",
      "Starting training ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                          | 1/5413 [00:00<17:34,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▋                                    | 501/5413 [01:29<13:48,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████▏                               | 1001/5413 [02:57<13:00,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████▊                            | 1502/5413 [04:24<09:03,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████▍                        | 2001/5413 [06:08<11:46,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|██████████████████                     | 2501/5413 [07:56<10:15,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████▌                 | 3001/5413 [09:44<09:14,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|█████████████████████████▏             | 3501/5413 [11:35<06:29,  4.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████▊          | 4001/5413 [13:23<06:14,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████▍      | 4501/5413 [15:15<03:17,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|████████████████████████████████████   | 5001/5413 [17:11<01:32,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5413/5413 [18:37<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Starting batch 500\n",
      "Starting batch 1000\n",
      "Starting batch 1500\n",
      "Starting batch 2000\n",
      "Epoch train loss : 4.000779066203857| Time: 23m 52s\n",
      "Epoch valid loss: 5.487070748054269\n",
      "Epoch EM: 31.759697256385998\n",
      "Epoch F1: 42.93662796486243\n",
      "====================================================================================\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "ems = []\n",
    "f1s = []\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_dataset)\n",
    "    valid_loss, em, f1 = valid(model, valid_dataset)\n",
    "    \n",
    "#     writer.add_scalar('train_loss', train_loss, epoch)\n",
    "#     writer.add_scalar('valid_loss', valid_loss, epoch)\n",
    "    \n",
    "    \n",
    "#     for name, param in model.named_parameters():\n",
    "#         writer.add_histogram(name, param, epoch)\n",
    "    \n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': valid_loss,\n",
    "            'em':em,\n",
    "            'f1':f1,\n",
    "            }, 'bidaf_run4_{}.pth'.format(epoch))\n",
    "    \n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    ems.append(em)\n",
    "    f1s.append(f1)\n",
    "\n",
    "    print(f\"Epoch train loss : {train_loss}| Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"Epoch valid loss: {valid_loss}\")\n",
    "    print(f\"Epoch EM: {em}\")\n",
    "    print(f\"Epoch F1: {f1}\")\n",
    "    print(\"====================================================================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['squad_train.json', 'squad_dev.json']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('./data/Squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* Papers read/referred:\n",
    "    1. BiDAF: https://arxiv.org/abs/1611.01603\n",
    "    2. Convolutional Neural Networks for Sentence Classification: https://arxiv.org/abs/1408.5882\n",
    "    3. Highway Networks: https://arxiv.org/abs/1505.00387\n",
    "* Other helpful links:\n",
    "    1. https://nlp.seas.harvard.edu/slides/aaai16.pdf. A great resource for character embeddings. The figures in the character embedding section are taken from here.\n",
    "    2. https://towardsdatascience.com/the-definitive-guide-to-bi-directional-attention-flow-d0e96e9e666b. A great series of blogs to understand BiDAF.\n",
    "    Some of the following repos might be out of date.\n",
    "    3. https://github.com/allenai/bi-att-flow\n",
    "    4. https://github.com/galsang\n",
    "    5. https://github.com/jojonki/BiDAF/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
